# -*- coding: utf-8 -*-
"""Another copy of AI_Powered_Multilingual_model1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x4LaKiD9lsFocDtIjNvhWjNKsWRqg1Ez
"""

In todayâ€™s globally connected world, the ability to communicate across language barriers is essential for education, business, travel, and social interaction. This project aims to address this need by developing an AI-powered multi-language translator using modern transformer-based neural networks.

At the core of the system is Facebookâ€™s M2M100 model â€” one of the first many-to-many multilingual machine translation models, capable of directly translating between any pair of over 100 languages without needing to pivot through English. Unlike traditional translation systems that require separate models for each language pair, M2M100 handles many-to-many translation efficiently within a single unified model.

The translator is integrated with a Gradio-based web interface, which makes the system user-friendly and interactive. Gradio allows users to input text, select source and target languages, and receive real-time translation output â€” all accessible via a simple web link directly from a Jupyter Notebook or Google Colab.



Key features:
âœ… Supports multiple languages (English, Hindi, French, German, Spanish, Chinese, Japanese, Korean, and more)
âœ… Powered by state-of-the-art transformer architecture (M2M100)
âœ… Fully serverless and runs directly in Google Colab
âœ… Interactive and shareable web interface built with Gradio
âœ… No special hardware or deployment server required

# âœ… Install dependencies
!pip install transformers sentencepiece gradio

import os
from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer
import gradio as gr

os.environ.pop("HUGGINGFACE_TOKEN", None)

# âœ… Load tokenizer and model
model_name = "facebook/m2m100_418M"

tokenizer = M2M100Tokenizer.from_pretrained(model_name)
model = M2M100ForConditionalGeneration.from_pretrained(model_name)

print("âœ… Model and tokenizer loaded!")

def translate(text, src_lang, tgt_lang):
    tokenizer.src_lang = src_lang
    encoded = tokenizer(text, return_tensors="pt")
    generated_tokens = model.generate(
        **encoded,
        forced_bos_token_id=tokenizer.get_lang_id(tgt_lang)
    )
    translated = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
    return translated[0]

src_langs = [
    "en", "hi", "fr", "de", "es", "zh", "ja", "ko"
]

with gr.Blocks() as demo:
    gr.Markdown("# ðŸŒ Multi-Language Translator (M2M100)")
    with gr.Row():
        input_text = gr.Textbox(label="Enter text to translate")
    with gr.Row():
        src = gr.Dropdown(choices=src_langs, value="en", label="Source Language")
        tgt = gr.Dropdown(choices=src_langs, value="hi", label="Target Language")
    with gr.Row():
        translate_btn = gr.Button("Translate")
    output = gr.Textbox(label="Translation Output")

    translate_btn.click(translate, inputs=[input_text, src, tgt], outputs=output)

# ðŸŸ¢ Launch it â€” works in Colab!
demo.launch(share=True)

hi_text = "à¤œà¥€à¤µà¤¨ à¤à¤• à¤šà¥‰à¤•à¤²à¥‡à¤Ÿ à¤¬à¥‰à¤•à¥à¤¸ à¤•à¥€ à¤¤à¤°à¤¹ à¤¹à¥ˆà¥¤"
chinese_text = "ç”Ÿæ´»å°±åƒä¸€ç›’å·§å…‹åŠ›ã€‚"

tokenizer.src_lang = "hi"
encoded_hi = tokenizer(hi_text, return_tensors="pt")
generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id("fr"))
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
# => "La vie est comme une boÃ®te de chocolat."

# translate Chinese to English
tokenizer.src_lang = "zh"
encoded_zh = tokenizer(chinese_text, return_tensors="pt")
generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id("en"))
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
# => "Life is like a box of chocolate."
